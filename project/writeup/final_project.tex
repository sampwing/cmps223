
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title{Improving Spam Filtering using NLP}
\author{Sam Wing \\ sampwing@soe.ucsc.edu}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle

\begin{abstract}
In this paper I describe the use of Machine Learning and Natural Language Processing Algorithms for classifying spam emails. Detecting spam in emails versus say websites was chosen because I am more interested in working with the Natural Language Understanding of the content of emails. I chose to use the Ling-Spam \cite{bayesianfiltering} due to the data being preprocessed, with all of the header information removed and the content cleaned of any unicode or html, allowing me to focus on the NLP aspects of this problem. Additionally, using this dataset gives me a baseline for which to evaluate my results. After performing some feature extraction on the Ling-Spam corpus, I ran some experiments to determine which model would work best for classifying spam.  I then used this model to implement a spam filter than can be hooked up to an email box via the IMAP protocol for classifying spam online. Project can be found at: \\https://github.com/sampwing/cmps223/tree/master/project/spam\_filter.py
\end{abstract}

\section{Introduction}
Spam filtering is an important topic in computer security, but not because all spam is malicious.  There does however exist a subset of these messages which are actually phishing attacks.  A phishing attack\cite{wikiphishing} is when an attacker poses as a reliable source or company, and attempts to gain personal information about a victim such as user names, passwords, credit card information, or any other number of otherwise hidden attributes.  Attackers can sometimes create lists of these user name / password combinations to be used for attacking vulnerable websites.  While not a phishing attack we have seen this time and time again, such as the leak of information from Sony, or the more recent leak of these credentials from LinkedIn.  Apparently, often times some users will use the same credentials for multiple sites -which is how this information can be used for cross site attacks.

In this paper I will refer to legitimate emails, which the user most likely wants to receive as ham, all other messages will be referred to as spam.  In spam filtering it is important to correctly identify ham messages as such so that they are not filtered, it is more important that a user receives all of their ham messages and letting a few spam messages pass through than having 100\% recall on spam items but filtering some of the legitimate ham messages.

There are a number of ways modern spam filters attempt to distinguish between spam and ham, they are often used in conjunction with one another to achieve high levels of performance.  One of the most elegant ways is through means of OCR, when text is extracted from images embedded in these emails and is then passed off to some other classifier to determine if the text contained within the images is ham or spam.  Another method is by looking at the header of an email.  The header contains a variety of information but one of the more useful fields found in the header is the originating source of the email.  There exist white and black lists which contain known ham and spam web addresses appropriately.  Spam filters are able to use this originating source information to determine if it belongs to either of these two lists to help in classifying a message as spam.  The method I am focusing on is the plain text which is sent in the body, that most people are likely familiar with when talking about email.

I present a number of features which have been successfully used in Natural Language Processing for text classification tasks which I hope to use in my experiments to gain a better performance over some existing techniques that deal with processing this plain text found in the body of emails.  I will use these features and a number of classifiers to determine what the best model is given my data and feature set, and use this model in my own implementation of a spam filter which will work on filtering emails it reads through the IMAP protocol.

\section{Previous Work}

	One of the first papers on automatically identifying spam was the work done by Androutsopoulos et al. \cite{bayesianfiltering}.  Their work is inspired by the work of McCallum et. al.\cite{naivebayes}, who had used Naive Bayes for text classification.  I used the Ling-Spam dataset which was used in this paper because it is fairly naive in it's assumption that Bag of Words model is the best feature to use for purely content based feature extraction, while disregarding non-textual information.  There have been some improvements in computational linguistics in the last decade and I hope to try to improve on the performance of automatic classification found in this paper.
	
	I also looked at the work done by Fette et al \cite{phishing}, for automatically filtering phishing attempts out from email.  Their approach assumed that the problem of identifying spam was solved, they had a boolean feature used in training their model which specific whether a particular email was spam or ham before deriving additional features..  They did however utilize a few features based on the content, which consisted of essentially counting the number of links within emails, and further analyzing each of those links by the number of subdomains contained within each link.  This paper used SVMs, I chose to follow suite and add SVMs as an additional classifier to identify spam messages in my project.

\section{Feature Extraction}

	There were a number of ideas from the papers I read for this project in addition to my own work in NLP that lead me to a variety of feature extraction on the content of the emails found in the Ling-Spam corpus.  The first of which was to create a Bag of Words model, specifying each token as a boolean whether it had occurred or not\cite{pang}, which was described in a number of papers \cite{bayesianfiltering} \cite{phishing} I used as a baseline to compare against a number of classifiers with a combination of additional features.  The Bag of Words model had some additional preproccessing which included passing it through a Snowball Stemmer \cite{snowball}.  Stemmers are used to reduce the dimensionality of text processing problems, and by collapsing several correlated words into a single group can improve performance of the classifier.
	
	Another group of features which were extracted were the LIWC features.  The LIWC is a dictionary of mappings from words to up to seventy different categories. Current the LIWC maps 2320 word stems to up to seventy different categories.  These categories include groups such as pronouns, affect, cognitive mechanisms, social words, amongst a number of other useful distinct groupings. My hope is that these categories and the words that map into them, will have a different distribution between spam and ham which may enable the classifier to improve its performance. 
	
	WordNet Domains is another tool which was used to extract an additional set of categories not covered by the LIWC. WordNet Domains consists of 164, hierarchically organized,\cite{wordnet} domain labels.  Containing such groups such as Sports, Buildings, etc.  This tools allows us to further reduce the dimensionality of the documents and examine the similarities between the documents that we would have not been able to without this semantic information.
	
	The last feature set that was derived using MSOL \cite{msol}.  This is a tool which maps verbs and nouns into either a positive or negative polarity domains.  It has been researched heavily that content polarity can be useful in a variety of text classification problems \cite{sentiment}, so I figured I would attempt to use it in spam identification.

\section{Experiment}

	The dataset which was used for the training and testing the spam filter that I created was the Ling-Spam dataset.  The Ling-Spam dataset consists of 2412 ham messages, and 481 spam messages.   Spam is 16.6\% of the corpus.  I chose to use three different classifiers, Naive Bayes, SVM with a linear kernel, and Logistic Regression.  I didn't come across papers which used Logistic Regression for Spam Filtering, but I have personally found Logistic Regression to handle text classification tasks fairly well.  	Each test was performed with three fold cross validation, this number was chosen do to the sparsity of the spam messages when compared to ham which was a one to six ratio.  I wanted the classifier to have enough spam messages to train on to get accurate scores. In addition to using cross validation, I employed the use of $\chi^2$ feature selector to reduce the dimensionality of the vector space even more.  This greatly improves the performance of classifiers like SVM, which tries to find a soft linear separation between all data points, and reducing the dimensionality greatly speeds up this process.


\section{Results}

After performing the experiments I generated a table of my results in Table \ref{tab:results}.  The features I chose to show were the better performing features, bow-desc is the bag of word model that discriminates between subject and body.  Polarity is any data gathered from running MSOL over the data. WN is running WordNet Domains over the textual content.  The first thing I found strange was the different between Table \ref{tab:results} and Table \ref{tab:lingspam}, I was curious why they were able to get better Precision and Accuracy scores than me.  After thinking about it however, I believe it is because they used a different stemmer than I did.  I used the Snowball stemmer trained on the english language, whereas they used the GATE Naive Bayes and Stemmer, which is trained on British English.  The Snowball implementation I used did not support British English.

\begin{table}[h]
\centering
\begin{tabular}{| l | l | r | r | r |}
	\hline
	Classifier & Features & Precision & Recall & Accuracy \\
	\hline
	 Logistic & baseline &99.14\% & 96.67\% & 99.41\% \\
	 & bow-desc &{\bf 99.35\%} & 96.67\% & 99.44\% \\
	 & bow-desc-liwc &97.44\% & 94.59\% & 98.92\% \\
	 & bow-desc-polarity &99.14\% & 96.88\% & 99.44\% \\
	 & bow-desc-wn &{\bf 99.35\%} & 96.67\% & 99.44\% \\
	 & bow-desc-wn-polarity &99.14\% & 96.88\% & {\bf 99.48\%} \\
	 \hline
	 Naive Bayes & baseline &98.13\% & {\bf 98.54\%} & 99.37\% \\
	 & bow-desc &97.73\% & {\bf 98.54\%} & 99.27\% \\
	 & bow-desc-liwc &98.54\% & 98.33\% & 99.2\% \\
	 & bow-desc-polarity &98.13\% & {\bf 98.54\%} & 99.3\% \\
	 & bow-desc-wn &97.93\% & {\bf 98.54\%} & 99.27\% \\
	 & bow-desc-wn-polarity &98.33\% & {\bf 98.54\%} & 99.34\% \\
	 \hline
	 SVM Linear& baseline &98.72\% & 95.84\% & 98.99\% \\
	 & bow-desc &98.92\% & 95.63\% & 98.99\% \\
	 & bow-desc-liwc &96.79\% & 93.97\% & 98.75\% \\
	 & bow-desc-polarity &99.13\% & 95.63\% & 99.03\% \\
	 & bow-desc-wn &98.92\% & 95.84\% & 98.99\% \\
	 & bow-desc-wn-polarity &99.13\% & 95.63\% & 99.03\% \\
	\hline
\end{tabular}
\caption{Experiment results using a variety of classifiers and features}
\label{tab:results}
\end{table}


However just looking at the differences in my results in Table /ref{tab:results}.  It is apparent that using these features, SVM with a linear kernel is pretty much useless, well not useless but not as performant as the other classifiers with this feature set and corpus.  I am able to get the best spam recall with Naive Bayes at 98.5\%, which means that we correctly classified that amount of our actual spam messages.  Several of the other feature sets perform as well recall-wise, but this is simply due to the $\chi^2$ feature selection choosing the bag of words model on each iteration through the experiment.  The Logisitc Regression classifier get the best Accuracy when I threw all of the features at it, however it is not statistically significantly better than the baseline model for Logistic Regression.  The same could be said about its Precision scores.  For this reason, I chose to use the Naive Bayes Bag of Words model for implementing my Spam Filter.

\begin{table}[h]
\centering
\begin{tabular}{| l | r | r | r |}
	\hline
	& Prec & Recall & Accuracy \\
	\hline
	Performance & 100\% & 63.67\% & 99.993\% \\
	\hline
\end{tabular}
\caption{Results of Androutsopoulos et. al. on the Ling-Spam dataset, stemming a bag of words model}
\label{tab:lingspam}
\end{table}

\section{Conclusion}
While unfortunate that my original intuition remains unfounded that I could increase the performance of spam filters working on a content based level, it is worth mentioning that when accuracy is as high as it is 99.4\% it is extremely difficult to get any statistically significant gains.  I believe this is why current work in this field is focusing on other methods of identifying spam, such as looking at the information in the headers to identify domains from the blacklists, and the other hot topic of using OCR to extract additional content from the embedded images of emails.

\section{Implementation of Spam Filter}

	After running the experiments I chose to use the Bag of Words model that didn't discriminate between subject and body. to create the spam classifier to be used in my spam filter.  The machine learning algorithms were from a python package called Sci-Kit-Learn\cite{sklearn}.  I chose to use the IMAP protocol for connecting to the email server, since both my school and personal email addresses are through GMAIL which supports the IMAP protocol.  Additionally, I used the python library email for parsing the RFC822 specification to retrieve the content information, consisting of the subject and body of the email.  Although I was able to grab the header information if I wanted, I chose to use the same textual content information that was available in the Ling-Spam dataset, so that I wouldn't have to tweak the classifier.
	
	When running my program, it is assumed that you have a gmail account.  You will be prompted for your user name and password before your mailbox begins to download and run through the spam filter.  Messages classified as spam will be placed into the spam filter, and ham messages placed into the ham folder.  I have found that running this on my own email I get poor results.  This is likely due to the training data being over a decade old.  I had wanted to retrain the classifier using the spam collected  for my emails by google, but apparently these emails are deleted after thirty days, so there was not enough spam to retrain.

	My final project can be found at: \\ https://github.com/sampwing/cmps223/tree/master/project/spam\_filter.py
\section{Future Work}
	The work I focused on was the purely textual information given in the body of an email.  However there is other work being done on extracting further textual information from the images which can be embedded in emails, which allows the attacker to circumvent any purely textual analysis of spam messages.  Fumera et al. \cite{spamimages} describe a process similar to how I wished to approach the problem of spam filtering.  However instead of looking at the text found within the body of the emails to analyze, instead they focus on using OCR software to extract textual information from images embedded within emails.  Working with image data would entail more of an understanding of computer vision than I currently have, as well as finding an annotated corpus to use in identifying the text found in such images.


\bibliographystyle{plain}
\bibliography{biblio}

\end{document}